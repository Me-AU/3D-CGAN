{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import utils\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.ipynb (relevant cells)\n",
    "\n",
    "template_obj_path = \"path/to/symmetrized_1010134946_cleaned.obj\"  # example file\n",
    "fixed_faces = utils.load_fixed_connectivity(template_obj_path)\n",
    "# Save fixed_faces for visualization or exporting generated meshes.\n",
    "\n",
    "# Set parameters\n",
    "NUM_VERTICES = 3000       # as defined in models.py\n",
    "epoch_count = 400\n",
    "batch_size = 128\n",
    "noise_size = 200\n",
    "d_lr = 0.00005  # Discriminator learning rate\n",
    "g_lr = 0.0025   # Generator learning rate\n",
    "log_folder = \"logs/\"\n",
    "condition_size = 10  # Adjust to the number of SNP features in your CSV\n",
    "\n",
    "# Load your DNA and face mesh data\n",
    "csv_file = \"path/to/dna.csv\"        # Update with your CSV file path\n",
    "face_folder = \"path/to/face_folder\"   # Folder containing OBJ files\n",
    "snps, faces, ids = utils.load_dna_face_data(csv_file, face_folder)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Create a dataset where each sample is (SNP vector, mesh vertices)\n",
    "train_set = TensorDataset(torch.from_numpy(snps).float(), torch.from_numpy(faces).float())\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Model instantiation\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = models.Generator(noise_size=noise_size, condition_size=condition_size)\n",
    "discriminator = models.Discriminator(condition_size=condition_size)\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Optimizers and Loss\n",
    "import torch.optim as optim\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=d_lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=g_lr, betas=(0.5, 0.999))\n",
    "\n",
    "from torch.autograd import Variable\n",
    "criterion_GAN = torch.nn.BCELoss()\n",
    "\n",
    "def get_gan_loss(tensor, ones):\n",
    "    target = torch.ones_like(tensor.data) if ones else torch.zeros_like(tensor.data)\n",
    "    target = Variable(target.to(device), requires_grad=False)\n",
    "    return criterion_GAN(tensor, target)\n",
    "\n",
    "def get_noise(b_size=batch_size):\n",
    "    return torch.randn([b_size, noise_size], device=device)\n",
    "\n",
    "# Training Loop for one epoch (for mesh data)\n",
    "def train_GAN_epoch():\n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    gen_out = []\n",
    "    \n",
    "    for snp, mesh in train_loader:\n",
    "        snp = snp.to(device)\n",
    "        mesh = mesh.to(device)\n",
    "        \n",
    "        # ---- Train Discriminator ----\n",
    "        discriminator.zero_grad()\n",
    "        real_output = discriminator(mesh, snp)\n",
    "        errD_real = get_gan_loss(real_output, True)\n",
    "        \n",
    "        noise = get_noise(mesh.shape[0])\n",
    "        fake_mesh = generator(noise, snp)\n",
    "        fake_output = discriminator(fake_mesh.detach(), snp)\n",
    "        errD_fake = get_gan_loss(fake_output, False)\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # ---- Train Generator ----\n",
    "        generator.zero_grad()\n",
    "        noise = get_noise(mesh.shape[0])\n",
    "        fake_mesh = generator(noise, snp)\n",
    "        output = discriminator(fake_mesh, snp)\n",
    "        errG = get_gan_loss(output, True)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        d_loss.append(errD.item())\n",
    "        g_loss.append(errG.item())\n",
    "        gen_out.append(fake_mesh.detach().cpu())\n",
    "        \n",
    "    return np.mean(d_loss), np.mean(g_loss), gen_out\n",
    "\n",
    "import os\n",
    "\n",
    "def inference_test_set(generator, test_loader, fixed_faces, output_dir, noise_size, device):\n",
    "    \"\"\"\n",
    "    Inference on the test set: for each sample, generate mesh vertices using the generator\n",
    "    and export the complete mesh (vertices + fixed_faces) as an OBJ file.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (snp, _) in enumerate(test_loader):\n",
    "            snp = snp.to(device)\n",
    "            noise = torch.randn([snp.size(0), noise_size], device=device)\n",
    "            generated_mesh = generator(noise, snp)  # generated_mesh: (batch, NUM_VERTICES, 3)\n",
    "            # Export each generated mesh in the batch\n",
    "            for i in range(generated_mesh.size(0)):\n",
    "                vertices = generated_mesh[i].cpu().numpy()\n",
    "                file_path = os.path.join(output_dir, f\"generated_mesh_{batch_idx * test_loader.batch_size + i}.obj\")\n",
    "                utils.export_mesh_to_obj(vertices, fixed_faces, file_path)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.ipynb (dataset split example)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Assume 'snps' and 'faces' have been loaded via utils.load_dna_face_data(...)\n",
    "full_dataset = TensorDataset(torch.from_numpy(snps).float(), torch.from_numpy(faces).float())\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_set, test_set = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "d_list = []\n",
    "g_list = []\n",
    "output_visualization_dir = os.path.join(log_folder, \"generated_objs\")\n",
    "os.makedirs(output_visualization_dir, exist_ok=True)\n",
    "\n",
    "import os\n",
    "log_folder = \"logs/\"\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "log_file = open(os.path.join(log_folder, \"logs.txt\"), \"a\")\n",
    "\n",
    "pbar = tqdm(range(epoch_count + 1))\n",
    "for epoch in pbar:\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Train one epoch\n",
    "    d_loss, g_loss, gen_out = train_GAN_epoch()\n",
    "    \n",
    "    d_list.append(d_loss)\n",
    "    g_list.append(g_loss)\n",
    "    \n",
    "    # Log losses and time\n",
    "    epoch_time = time.time() - startTime\n",
    "    log_string = f\"Epoch {epoch} --> D_loss: {d_loss:.3f} | G_loss: {g_loss:.3f} | Time: {epoch_time:.3f}\"\n",
    "    pbar.set_description(log_string)\n",
    "    log_file.write(log_string + \"\\n\")\n",
    "    log_file.flush()\n",
    "    \n",
    "    # Periodic visualization: export one generated mesh as an OBJ\n",
    "    if epoch % 10 == 0:\n",
    "        # For visualization, take one batch from the training set (or a dedicated test_loader)\n",
    "        noise = get_noise(batch_size)\n",
    "        # Here, using the first batch from the training loader as a demo:\n",
    "        try:\n",
    "            snp, _ = next(iter(train_loader))\n",
    "        except StopIteration:\n",
    "            continue\n",
    "        snp = snp.to(device)\n",
    "        generated_mesh = generator(noise, snp)\n",
    "        # Export the first sample of the batch\n",
    "        vertices = generated_mesh[0].cpu().numpy()\n",
    "        vis_file_path = os.path.join(output_visualization_dir, f\"generated_mesh_epoch_{epoch}.obj\")\n",
    "        utils.export_mesh_to_obj(vertices, fixed_faces, vis_file_path)\n",
    "        \n",
    "# After training, perform full inference on the test set and export results.\n",
    "# Assume 'test_loader' is defined similar to train_loader for the test split.\n",
    "inference_output_dir = os.path.join(log_folder, \"test_inference_objs\")\n",
    "inference_test_set(generator, test_loader, fixed_faces, inference_output_dir, noise_size, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
